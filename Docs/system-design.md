1. Overview

Second Brain AI is a privacy-first, temporal, multimodal AI system designed to ingest, store, and reason over a user’s personal knowledge.
Unlike traditional chatbots, the system emphasizes memory, time awareness, and retrieval accuracy over conversational fluency.

The system enables users to ask natural language questions such as:

“What were the key concerns discussed in last Tuesday’s meeting?”

“Summarize the article I saved about quantum computing.”

Answers are generated by retrieving the most relevant historical context and synthesizing it using an LLM (Retrieval-Augmented Generation, RAG).

2. High-Level Architecture
Components

Frontend: Lightweight chat UI (HTML + JS)

API Layer: ASP.NET Core 8 REST APIs

Ingestion Pipeline: Asynchronous background processing

Embedding Service: Pluggable (fake or OpenAI)

Storage: PostgreSQL + pgvector

Retrieval Engine: Hybrid semantic + temporal

LLM Layer: Context synthesis (optional / pluggable)

Architecture Diagram (Logical)
[Browser UI]
   |
[ASP.NET Core API]
     |
     --> Ingestion Controller
     |        |
     | 
     |   [Async Workers]
     |        |
     |   [Chunking + Embedding]
     |        |
     |   [PostgreSQL + pgvector]
     |
     --> Query Controller
              |
              
       [Hybrid Retrieval]
              |
              
         [LLM Synthesis]
              |
              
          [Answer]

3. Multi-Modal Data Ingestion Pipeline
Supported Modalities
Modality	Processing Strategy
Plain Text	Direct chunking
Documents (PDF, MD)	Text extraction + metadata
Audio (mp3, m4a)	Transcription → text
Web Content	URL scraping → cleaned text
Images (Proposed)	OCR + captioning
Ingestion Flow (All Modalities)

Input received (file, URL, text)

Text extraction / transcription

Chunking

Embedding generation

Persistence with timestamps

Raw Input
   |
Text Extraction / Transcription
   |
Chunking (semantic boundaries)
   |
Embedding Generation
   |
PostgreSQL (Chunks + Vectors)

Why Asynchronous Processing?

Avoids blocking user requests

Enables retries and fault tolerance

Scales independently of query traffic

4. Chunking Strategy
Rationale

LLMs and vector databases perform best with semantically coherent, bounded text units.

Strategy

Chunk size: ~300–500 tokens

Preserve semantic boundaries (paragraphs / sentences)

Each chunk is independently searchable

Stored Metadata per Chunk

chunk_id

source_type (audio, doc, web)

source_id

observed_at (semantic time)

ingested_at (system time)

5. Data Indexing & Storage Model
Why PostgreSQL + pgvector?

ACID guarantees

Native temporal queries

Vector similarity + relational joins

Lower operational complexity vs external vector DBs

Core Tables
Chunks
Column	Purpose
ChunkId (PK)	Unique chunk
Text	Chunk content
ObservedAt	When content occurred
SourceType	Audio / Doc / Web
Metadata	JSON metadata
Embeddings
Column	Purpose
EmbeddingId (PK)	Unique vector
ChunkId (FK)	Associated chunk
Vector (pgvector)	Semantic embedding
Indexing

pgvector index for semantic similarity

B-tree index on observed_at

Optional full-text index for keyword fallback

6. Information Retrieval Strategy (Core Brain)
Chosen Approach: Hybrid Retrieval
Technique	Role
Semantic Search	Meaning-based relevance
Temporal Filtering	Time-based narrowing
Keyword Search (optional)	Precision fallback
Retrieval Flow

Generate query embedding

Apply time filters (observed_at)

Rank by vector similarity

Select top-K chunks

Pass context to LLM

ORDER BY embedding <=> query_vector

Why Hybrid Beats Vector-Only

Vector similarity alone ignores time

Keyword-only ignores semantics

Hybrid balances accuracy + explainability

7. Temporal Query Support (Key Design Feature)
Observed Time vs Ingestion Time
Timestamp	Meaning
observed_at	When the event/content happened
ingested_at	When system processed it

All retrieval is based on observed_at.

This allows queries like:

“What did I work on last month?”

“What was discussed before the deadline?”

Temporal Query Example
WHERE observed_at BETWEEN :from AND :to


Temporal constraints are applied before similarity ranking, ensuring semantic relevance within the correct timeframe.

8. LLM-Based Answer Synthesis (RAG)
Approach

Retrieve top-K relevant chunks

Concatenate into a structured context

Prompt LLM with:

User question

Retrieved evidence

Instruction to synthesize

Why RAG?

Prevents hallucinations

Grounds answers in user-owned data

Maintains explainability

9. Scalability Considerations
Per-User Scaling

Partition by user_id

Row-level security (RLS)

Embedding cache reuse

Storage Scaling

Postgres vertical scaling

Read replicas for queries

Vector index pruning

Ingestion Scaling

Independent worker pools

Backpressure via queues

10. Privacy by Design
Design Principles

User owns all data

No training on user data

Optional local-first deployment

Trade-Offs
Cloud	Local
Easier scaling	Maximum privacy
Managed infra	User-controlled
Higher trust requirement	Offline capability

The architecture supports both with minimal changes.

11. Trade-Off Analysis
Decision	Trade-Off
PostgreSQL vs Vector DB	Simplicity over extreme scale
Chunking	More storage, better recall
RAG	Higher latency, higher accuracy
Async ingestion	Complexity, resilience
12. Future Enhancements

Streaming token responses

Knowledge graph relationships

Image OCR + caption embeddings

Personalized ranking

Client-side encryption

13. Conclusion

This system is intentionally designed as a memory-first AI, prioritizing correctness, time awareness, and architectural clarity over superficial conversational polish.

The design balances:

Semantic understanding

Temporal reasoning

Privacy

Scalability

Maintainability

It serves as a strong foundation for a production-grade personal AI companion.
